{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec6dd41",
   "metadata": {},
   "source": [
    "# Notebook de choix de modèle de classification binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25801800",
   "metadata": {},
   "source": [
    "Notebook ChatGPT : \n",
    "\n",
    "**Modèles à comparer**\n",
    "\n",
    "a) Modèles classiques (baseline)\n",
    "\n",
    "* Logistic Regression\n",
    "* Decision Tree\n",
    "* Random Forest\n",
    "* Gradient Boosting (XGBoost / LightGBM / CatBoost)\n",
    "* Support Vector Machine (SVM) avec kernel linéaire et RBF)\n",
    "\n",
    "b) Modèles linéaires avancés\n",
    "\n",
    "* Ridge / Lasso / ElasticNet\n",
    "* Stochastic Gradient Descent (SGDClassifier)\n",
    "\n",
    "c) Réseaux de neurones simples\n",
    "\n",
    "* MLP (1 à 3 couches cachées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "763325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Lasso, ElasticNet, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "import data_preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10711cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/train.csv\", sep=\",\")\n",
    "\n",
    "X, _ = data_preparation.prepare_df(df.drop(columns=[\"Survived\", \"PassengerId\"]))\n",
    "y = df.loc[:, \"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8efb77",
   "metadata": {},
   "source": [
    "## Optimisation sur l'ensemble des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72769fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\dev\\titanic\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [11:28:44] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 268, number of negative: 444\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000339 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 712, number of used features: 8\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.376404 -> initscore=-0.504838\n",
      "[LightGBM] [Info] Start training from score -0.504838\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\dev\\titanic\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Normalisation pour les modèles linéaires et SVM\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Dictionnaire des modèles\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"LightGBM\": lgb.LGBMClassifier(),\n",
    "    \"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "    \"SVM\": SVC(),\n",
    "    \"Ridge\": RidgeClassifier(),\n",
    "    \"Lasso\": SGDClassifier(loss='squared_hinge', penalty='l1', max_iter=1000),  # Lasso adapté pour classification\n",
    "    \"ElasticNet\": SGDClassifier(loss='squared_hinge', penalty='elasticnet', max_iter=1000),\n",
    "    \"SGDClassifier\": SGDClassifier(max_iter=1000, tol=1e-3),\n",
    "    \"MLP\": MLPClassifier(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Choisir si on utilise les features normalisées\n",
    "    if name in [\"Logistic Regression\", \"SVM\", \"Ridge\", \"Lasso\", \"ElasticNet\", \"SGDClassifier\", \"MLP\"]:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7ce3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Accuracy = 0.8156\n",
      "Decision Tree: Accuracy = 0.7709\n",
      "Random Forest: Accuracy = 0.8268\n",
      "XGBoost: Accuracy = 0.8212\n",
      "LightGBM: Accuracy = 0.8101\n",
      "CatBoost: Accuracy = 0.8268\n",
      "SVM: Accuracy = 0.8156\n",
      "Ridge: Accuracy = 0.7877\n",
      "Lasso: Accuracy = 0.6425\n",
      "ElasticNet: Accuracy = 0.6760\n",
      "SGDClassifier: Accuracy = 0.7654\n",
      "MLP: Accuracy = 0.8492\n"
     ]
    }
   ],
   "source": [
    "for name, acc in results.items():\n",
    "    print(f\"{name}: Accuracy = {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59220cc2",
   "metadata": {},
   "source": [
    "## Optimisation sur les paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d42a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0237a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_results_from_rand_search(rand_search):\n",
    "    results = {\n",
    "        \"best_params\": rand_search.best_params_,\n",
    "        \"best_score\": float(rand_search.best_score_),  # convertir en float pour JSON\n",
    "        \"best_estimator\": str(rand_search.best_estimator_),  # ou str() pour une description\n",
    "        \"cv_results\": {}\n",
    "    }\n",
    "\n",
    "    # Extraire cv_results_\n",
    "    cv_results = rand_search.cv_results_\n",
    "    for key, value in cv_results.items():\n",
    "        # Convertir les arrays numpy en liste pour JSON\n",
    "        if isinstance(value, np.ndarray):\n",
    "            results[\"cv_results\"][key] = value.tolist()\n",
    "        else:\n",
    "            results[\"cv_results\"][key] = value\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce865748",
   "metadata": {},
   "source": [
    "### RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3813cf",
   "metadata": {},
   "source": [
    "#### a. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c47390b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Best params: {'activation': 'relu', 'alpha': np.float64(0.003062735057040824), 'hidden_layer_sizes': (50,), 'learning_rate': 'constant', 'solver': 'adam'}\n",
      "MLP Best CV accuracy: 0.8216560885957759\n"
     ]
    }
   ],
   "source": [
    "# 4min 06.3\n",
    "mlp = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "mlp_param_dist = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100,50), (50,50,25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': uniform(0.0001, 0.01),  # régularisation\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp_rand = RandomizedSearchCV(\n",
    "    mlp, mlp_param_dist, n_iter=30, cv=3, scoring='accuracy', n_jobs=-1, random_state=42\n",
    ")\n",
    "mlp_rand.fit(X_train_scaled, y_train)\n",
    "print(\"MLP Best params:\", mlp_rand.best_params_)\n",
    "print(\"MLP Best CV accuracy:\", mlp_rand.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829866f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/MLP_randomized_search_results.json\", \"w\") as f:\n",
    "    json.dump(extract_results_from_rand_search(mlp_rand), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11c832",
   "metadata": {},
   "source": [
    "#### b. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecb1abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Best params: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 140}\n",
      "RF Best CV accuracy: 0.8286529801794136\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rf_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "rf_rand = RandomizedSearchCV(rf, rf_param_dist, n_iter=30, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "rf_rand.fit(X_train, y_train)\n",
    "print(\"RF Best params:\", rf_rand.best_params_)\n",
    "print(\"RF Best CV accuracy:\", rf_rand.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a2f5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/RF_randomized_search_results.json\", \"w\") as f:\n",
    "    json.dump(extract_results_from_rand_search(rf_rand), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17590111",
   "metadata": {},
   "source": [
    "#### c. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "129bced4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Best params: {'colsample_bytree': np.float64(0.8834959481464842), 'learning_rate': np.float64(0.01070663052197174), 'max_depth': 3, 'n_estimators': 148, 'subsample': np.float64(0.8574323980775167)}\n",
      "XGBoost Best CV accuracy: 0.8328901180725454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\dev\\titanic\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [11:57:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': randint(100, 500),\n",
    "    'max_depth': randint(3, 10),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'subsample': uniform(0.7, 0.3),\n",
    "    'colsample_bytree': uniform(0.7, 0.3)\n",
    "}\n",
    "\n",
    "xgb_rand = RandomizedSearchCV(xgb_model, xgb_param_dist, n_iter=30, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "xgb_rand.fit(X_train, y_train)\n",
    "print(\"XGBoost Best params:\", xgb_rand.best_params_)\n",
    "print(\"XGBoost Best CV accuracy:\", xgb_rand.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09294e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/XGBoost_randomized_search_results.json\", \"w\") as f:\n",
    "    json.dump(extract_results_from_rand_search(xgb_rand), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37a80da",
   "metadata": {},
   "source": [
    "#### d. CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23fe0d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Best params: {'border_count': 100, 'depth': 4, 'iterations': 204, 'l2_leaf_reg': 2, 'learning_rate': np.float64(0.033598491974895575)}\n",
      "CatBoost Best CV accuracy: 0.8328723894621138\n"
     ]
    }
   ],
   "source": [
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "cat_param_dist = {\n",
    "    'iterations': randint(200, 1000),\n",
    "    'depth': randint(4, 8),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'l2_leaf_reg': randint(1, 5),\n",
    "    'border_count': [32, 50, 100]\n",
    "}\n",
    "\n",
    "cat_rand = RandomizedSearchCV(cat_model, cat_param_dist, n_iter=30, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "cat_rand.fit(X_train, y_train)\n",
    "print(\"CatBoost Best params:\", cat_rand.best_params_)\n",
    "print(\"CatBoost Best CV accuracy:\", cat_rand.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7edb9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/CatBoost_randomized_search_results.json\", \"w\") as f:\n",
    "    json.dump(extract_results_from_rand_search(cat_rand), f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699dac2e",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32a10fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Best params: {'colsample_bytree': 0.8, 'learning_rate': 0.015, 'max_depth': 3, 'n_estimators': 160, 'subsample': 0.857}\n",
      "XGBoost Best accuracy: 0.8356643356643356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathi\\dev\\titanic\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [12:13:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [120, 140, 160, 180],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.008, 0.010, 0.012, 0.015],\n",
    "    'subsample': [0.857],\n",
    "    'colsample_bytree': [0.8, 0.88, 0.95]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "print(\"XGBoost Best params:\", xgb_grid.best_params_)\n",
    "print(\"XGBoost Best accuracy:\", xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "014cde37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/XGBoost_grid_search_results.json\", \"w\") as f:\n",
    "    json.dump(extract_results_from_rand_search(xgb_grid), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d33935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Best params: {'border_count': 100, 'depth': 5, 'iterations': 220, 'l2_leaf_reg': 3, 'learning_rate': 0.03}\n",
      "CatBoost Best accuracy: 0.8342558849601103\n"
     ]
    }
   ],
   "source": [
    "cat_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "cat_param_grid = {\n",
    "    'iterations': [180, 200, 220, 250],\n",
    "    'depth': [4, 5, 6],\n",
    "    'learning_rate': [0.025, 0.030, 0.035, 0.040],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'border_count': [100]\n",
    "}\n",
    "\n",
    "cat_grid = GridSearchCV(cat_model, cat_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "cat_grid.fit(X_train, y_train)\n",
    "print(\"CatBoost Best params:\", cat_grid.best_params_)\n",
    "print(\"CatBoost Best accuracy:\", cat_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0f4a313",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/CatBoost_grid_search_results.json\", \"w\") as f:\n",
    "    json.dump(extract_results_from_rand_search(cat_grid), f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
